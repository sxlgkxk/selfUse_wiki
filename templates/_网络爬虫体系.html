{% extends "layout.html" %}{% block body %}<h1><a href='/c/技术总结/linux技术词条/爬虫体系'>技术总结/linux技术词条/爬虫体系 </a> 网络爬虫体系 <i class='fas fa-edit btn_edit' style='color: #ddd' name='/v/技术总结/linux技术词条/爬虫体系/网络爬虫体系?action=edit' onclick='window.location.href=this.getAttribute("name")'></i></h1><hr/><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h3 id="知识点">知识点</h3>
<ul>
<li><a href="http://108.160.135.157:5000/v/chrome_headless">chrome_headless</a></li>
<li>解析网页
<ul>
<li>pyquery</li>
<li>正则</li>
</ul></li>
<li>请求方式
<ul>
<li>requests</li>
<li>表单</li>
<li>ip池
<ul>
<li><code>python proxyPool.py webserver</code></li>
<li><code>python proxyPool.py schedule</code></li>
</ul></li>
<li>浏览器伪装</li>
<li>访问频率</li>
<li>断线重连</li>
<li>ajax请求</li>
</ul></li>
<li>身份验证
<ul>
<li>验证码</li>
<li>cookies</li>
</ul></li>
<li>数据库存储</li>
<li>架构层优化
<ul>
<li>多线程</li>
<li>分布式</li>
<li>异步加载
<ul>
<li>asyncio
<ul>
<li>仅仅使用单线程, 就能达到多线程/进程的效果的工具</li>
</ul></li>
</ul></li>
</ul></li>
<li>新技术扩展</li>
<li>浏览器自动化
<ul>
<li><a href="https://www.v2ex.com/t/606683">v2ex教程</a></li>
<li>puppeteer
<ul>
<li><a href="https://try-puppeteer.appspot.com/">playground</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/30213568">zhihu教程</a></li>
</ul></li>
</ul></li>
<li>scrapy高级爬虫
<ul>
<li>整合了爬取, 处理数据, 存储数据的一条龙服务</li>
</ul></li>
<li>tampermonkey</li>
<li><a href="https://github.com/facert/awesome-spider">awesome spider</a></li>
</ul>
<h3 id="sites">sites</h3>
<ul>
<li><a href="https://morvanzhou.github.io/tutorials/data-manipulation/scraping/">莫烦</a></li>
<li><a href="https://piaosanlang.gitbooks.io/spiders/09day/README9.html">gitbook</a></li>
<li><a href="https://www.zhihu.com/question/35461941">知乎问题，爬虫进阶</a></li>
<li>博客教程
<ul>
<li><a href="https://cuiqingcai.com/1052.html">link</a></li>
<li><a href="https://blog.csdn.net/kissazhu/article/details/80864877">scrcpy框架</a></li>
<li><a href="https://www.lizenghai.com/archives/1459.html">link</a></li>
<li><a href="https://www.jianshu.com/p/5033b4a107da">简书1</a></li>
<li><a href="https://www.jianshu.com/p/939cce6945d0">简书2</a></li>
<li><a href="https://juejin.im/post/5a3100c56fb9a045186abbe5">掘金</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21479334">知乎专栏</a></li>
</ul></li>
</ul>
<h1 id="高性能爬虫的设计与实现">高性能爬虫的设计与实现</h1>
<h2 id="关键词网络爬虫-高性能-数据挖掘">关键词：网络爬虫 高性能 数据挖掘</h2>
<h2 id="摘要数据挖掘是支撑信息社会飞速发展的关键性领域而网络爬虫技术则是数据挖掘领域的基础性技术如何应对当今海量繁杂的网络数据如何准确迅速地获取所需信息如何快速开发出高效的爬虫程序成了我们面临的一项挑战本文将先介绍网络爬虫的概念和基本结构再就请求方式身份验证网页解析数据库存储架构层优化这六个方面来对网络爬虫技术进行详细阐释提出各部分的优化措施同时将优化前后的性能进行比较">摘要：数据挖掘是支撑信息社会飞速发展的关键性领域，而网络爬虫技术则是数据挖掘领域的基础性技术。如何应对当今海量繁杂的网络数据，如何准确迅速地获取所需信息，如何快速开发出高效的爬虫程序，成了我们面临的一项挑战。本文将先介绍网络爬虫的概念和基本结构，再就请求方式、身份验证、网页解析、数据库存储、架构层优化这六个方面来对网络爬虫技术进行详细阐释，提出各部分的优化措施，同时将优化前后的性能进行比较。</h2>
<h2 id="引言">引言</h2>
<p>随着信息革命的不断推进，人工智能，云技术，物联网，5G等各种新的技术层出不穷，毫无疑问，信息技术已经成了推动当今社会飞速发展的一个重要因素。然而，所有前沿尖端的技术都需要以高效获取所需信息的强大能力为前提和基础，比如人工智能领域中，神经网络的准确性需要靠大量准确的数据集来保证。本文将先介绍网络爬虫的概念和基本结构，再就请求方式、网页解析、身份验证、数据库存储、架构层优化这六个方面来进行详细阐释，提出各部分的优化措施，同时将优化前后的性能进行比较。</p>
<h2 id="网络爬虫的基本结构">网络爬虫的基本结构</h2>
<p>网络爬虫，即一段计算机程序或脚本，它按照预先设定好的规则，机械地从互联网采集所需信息，并将信息整理并存储。网络爬虫是数据挖掘和搜索引擎的基础，一段爬虫程序设计的好坏将直接决定系统性能的开销、花费时间的长短、获取信息的准确性。一般而言，一个爬虫程序的运行流程包含三部分：1,给定种子URLs，将所有种子URL页面采集下来；2,解析获取到的页面，从中提取新的URL，并将这些URL放入爬取列表中；3,重复步骤1和2。然而，在基础爬虫的基础上还需要加入分布式、多线程等技术来满足性能和时间要求，加入浏览器伪装、proxy代理、访问频率控制、用户登陆、验证码破解等技术来避免爬虫被网站发现与屏蔽，此外为了满足稳定性的要求，我们还需要考虑断线重连等操作。这些都进一步增加了网络爬虫开发的复杂程度，可以说，随着互联网的飞速发展，网络爬虫技术也发生着日新月异的变换，曾经简单单一的网络爬虫结构已经难以满足当下的需求。</p>
<h2 id="网络爬虫技术的详细阐释">网络爬虫技术的详细阐释</h2>
<h3 id="请求方式">请求方式</h3>
<p>为了获取网页页面，我们需要向url代表的ip地址发送请求，服务器接收请求后会返回网页内容。最基本的请求形式是GET请求和POST请求，而网页内容一般是html文件或json文件等。 #### 浏览器伪装 脚本访问网站与通过浏览器访问不同，我们需要人为设置GET请求的User-Agent和Referer部分的内容，从而避免盗链检测，User-Agent可从浏览器控制台获取，通过User-Agent的设置，我们还可以控制获取Android, Windows, Linux, IOS等不同平台的网页版本，Referer需根据网站设置的浏览逻辑和跳转规则进行设置，设置为相应的URL链接。</p>
<h4 id="使用代理">使用代理</h4>
<p>网站检测到从同一个ip发来的段时间内的大量请求时，可能会将该ip封禁，因此我们需要维护一个ip池，从多个ip代理对网站进行访问。</p>
<h4 id="频率限制">频率限制</h4>
<p>为了避免机械化重复的访问模式被网站识别，我们可以人为地通过随机化和休眠等方式控制访问的时间间隔</p>
<h4 id="断线重连">断线重连</h4>
<p>断线重连即在访问超时后自动重新发起请求或将url放入爬虫列表的尾部</p>
<h4 id="ajax请求">ajax请求</h4>
<p>ajax技术（Asynchronous JavaScript and XML）用于构建动态网页，客户端发送ajax请求后，获取服务器发来的数据，并用javascript更新页面。由于ajax技术构建的动态页面具有数据量交换减少、网页加载迅速等优点，现代网页多采用ajax技术来，这使得我们难以通过url直接获取完整网页，需要我们使用浏览器控制台对ajax接口进行分析，通过访问特定的ajax接口得到特定数据类型</p>
<h4 id="url去重">url去重</h4>
<p>为避免将已爬取的网页再次加入队列中，我们需要对新获取的url进行判重操作，可用平衡树方法或Bloom Filter算法，Bloom Filter用一个很大的位数组表示一个集合，将域名通过hash操作映射到数组中。</p>
<h3 id="网页解析">网页解析</h3>
<p>如今网页的展现形式五花八门，然而它们都是由基本的html文件，css文件和javascript文件构成的，html文件负责组建网页的基本结构，css文件负责网页的渲染与美化，javascript文件负责网页的动态交互。我们需要遵循html规范来对html文件进行解析 要想从内容复杂的html文件中获取信息，可选择用XPath（XML Path Language）、CSS Selector或正则表达式等方式来进行模式匹配与内容提取</p>
<h3 id="身份验证">身份验证</h3>
<h4 id="用户登陆状态保存">用户登陆状态保存</h4>
<p>一般网站对用户身份的识别是靠服务器端识别请求携带的cookies信息实现的。因此，我们可先进行登陆操作，将网站发来的cookies数据保存，在每次发送请求时将cookies信息携带，网站便能够对用户身份加以识别。此外，我们还可以维护一个cookies池，通过加载不同的cookies来切换到不同的用户账户</p>
<h4 id="验证码破解">验证码破解</h4>
<p>常见的验证码种类主要有图形型、滑动型、点触型、九宫格型这几种，一般的破解手段有ORC识别、模式匹配、图像处理等，为了应对越来越复杂的验证码规则，还可运用机器学习技术，通过爬取大量验证码集中训练来保证准确率。此外，对于过于复杂的验证码，还可预留接口给操作人员，进行人工解码。</p>
<h3 id="数据库存储">数据库存储</h3>
<p>获取的结果可以存入自定义的文本文件、关系型数据库或非关系型数据库</p>
<h3 id="并行优化">并行优化</h3>
<p>为了减少时间消耗，爬虫常采用并行优化，用空间和性能开销换取时间，并行常采用多线程或分布式架构这两种形式。多线程优化即维护一个线程池，分布式架构常采用主从架构（Master-Slave）。 主从架构中，主节点负责派发任务，从节点接收任务并将执行结果返回给主节点，同时从主节点分配到新的任务 此外，还可使用MVC架构，用webGUI或一个管理脚本来对爬虫进行管理</p>
<h3 id="图形化爬取">图形化爬取</h3>
<p>除了通过分析网站结构，通过访问接口直接获取数据外，我们还可以通过操纵浏览器来获取网页，其优点是无需分析复杂的ajax接口，在网站如何加盐、对何种信息进行怎样的加密操作未知时仍能进行爬取，缺点则是爬取效率慢，图形化的渲染消耗了多余的处理器资源。常用的框架主要有Appium、Selenium、Headless Chrome等</p>
<h3 id="js代码混淆">js代码混淆</h3>
<p>网站为了避免接口暴露，可能会将html文件及javascript文件进行多层加密与多层混淆，这就需要我们进行耐心分析，先通过变量替换，编码转化等方式将Unicode字符转换为可读的代码，再对代码进行简化、重构、进行解耦、分析得到各函数功能、找到入口函数，从而完成逆向工程。只有通过对javascript代码的分析，我们才能知道如何对时间戳进行操作、如何排序、加盐。</p>
<h3 id="爬取过程中的道德规范">爬取过程中的道德规范</h3>
<p>爬虫在对一个网站进行访问时需占用网站的大量带宽资源和处理器资源，因此，一方面我们需要精简自己的爬虫程序，尽可能提高其爬取效率，只针对所需页面进行操作，另一方面，我们还需要控制并发数，避免在同一时间发出大量请求，避免网站负荷瞬间过大，应控制请求的时间间隔，通过随机化的间歇性访问既避免了爬虫被识别，也避免了网站的负荷过载</p>
<h2 id="结语">结语</h2>
</body>
</html>{% endblock %}